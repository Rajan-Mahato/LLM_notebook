{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q bitsandbytes datasets accelerate loralib\n!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n!pip install datasets # Make sure the datasets module is installed\n!pip install evaluate\n!pip install seqeval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-25T17:22:07.305961Z","iopub.execute_input":"2024-09-25T17:22:07.306260Z","iopub.status.idle":"2024-09-25T17:24:09.188294Z","shell.execute_reply.started":"2024-09-25T17:22:07.306222Z","shell.execute_reply":"2024-09-25T17:24:09.187146Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.0.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.25.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m964.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m781.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=bcca249908d5d1c5fad821b299ee39ef28de3f81e5cd5799069b66af009dfb74\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:24:09.190234Z","iopub.execute_input":"2024-09-25T17:24:09.190579Z","iopub.status.idle":"2024-09-25T17:24:09.479663Z","shell.execute_reply.started":"2024-09-25T17:24:09.190545Z","shell.execute_reply":"2024-09-25T17:24:09.478806Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbc1461e9e1f44fabfb10f5d86dcd131"}},"metadata":{}}]},{"cell_type":"code","source":"!nvidia-smi -L","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:24:18.686808Z","iopub.execute_input":"2024-09-25T17:24:18.687855Z","iopub.status.idle":"2024-09-25T17:24:19.699792Z","shell.execute_reply.started":"2024-09-25T17:24:18.687793Z","shell.execute_reply":"2024-09-25T17:24:19.698861Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"GPU 0: Tesla T4 (UUID: GPU-125376c2-cb66-fe53-f46e-1db22fb67e90)\nGPU 1: Tesla T4 (UUID: GPU-21d40538-4c3a-8990-ca9e-ce0b92cc8511)\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom evaluate import load as load_metric  # Corrected import for metrics\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback\nimport torch","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:24:19.701561Z","iopub.execute_input":"2024-09-25T17:24:19.701917Z","iopub.status.idle":"2024-09-25T17:24:38.383625Z","shell.execute_reply.started":"2024-09-25T17:24:19.701883Z","shell.execute_reply":"2024-09-25T17:24:38.382765Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\n# Load the tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"GroNLP/bert-base-dutch-cased\")\nmodel = AutoModelForTokenClassification.from_pretrained(\n    \"GroNLP/bert-base-dutch-cased\",\n    num_labels=9  # Number of labels for the NER task\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:24:38.385082Z","iopub.execute_input":"2024-09-25T17:24:38.385682Z","iopub.status.idle":"2024-09-25T17:24:59.364591Z","shell.execute_reply.started":"2024-09-25T17:24:38.385646Z","shell.execute_reply":"2024-09-25T17:24:59.363678Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/254 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e16b23c5f32e4324986e01ef5c5bc80c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220a1ce536ce42fea426caa693835ccc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"280b24f000104ec6a2ffe588e79795d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50bcfc69a7294587b949bfea204ad501"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/437M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d34a64f9561546f486f6fa77a17bf13a"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at GroNLP/bert-base-dutch-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:24:59.366459Z","iopub.execute_input":"2024-09-25T17:24:59.367173Z","iopub.status.idle":"2024-09-25T17:24:59.372523Z","shell.execute_reply.started":"2024-09-25T17:24:59.367126Z","shell.execute_reply":"2024-09-25T17:24:59.371634Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nconfig = LoraConfig(\n    r=16, #attention heads\n    lora_alpha=32, #alpha scaling\n    # target_modules=[\"q_proj\", \"v_proj\"], #if you know the\n    lora_dropout=0.05,\n    bias=\"none\",\n     task_type=\"default\"  # Assuming 'default' is a neutral option\n)\n\nmodel = get_peft_model(model, config)\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:24:59.373826Z","iopub.execute_input":"2024-09-25T17:24:59.374140Z","iopub.status.idle":"2024-09-25T17:24:59.528496Z","shell.execute_reply.started":"2024-09-25T17:24:59.374108Z","shell.execute_reply":"2024-09-25T17:24:59.527539Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"trainable params: 589824 || all params: 109143561 || trainable%: 0.5404111745996633\n","output_type":"stream"}]},{"cell_type":"code","source":"# load data\ndata = load_dataset(\"procit008/training_dataset_ner_0.2\")\n# Get label list\nlabel_list = data[\"train\"].features[\"ner_tags\"].feature.names","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:25:01.002907Z","iopub.execute_input":"2024-09-25T17:25:01.003302Z","iopub.status.idle":"2024-09-25T17:25:10.712943Z","shell.execute_reply.started":"2024-09-25T17:25:01.003264Z","shell.execute_reply":"2024-09-25T17:25:10.712195Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.33k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b8e69c1589b48189dfdc98caa0e9ff6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/4.38M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"062cc9bfb44641b1b024f2a62f744315"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/716k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"047901d578134d1d8c6f660ba8496fcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/686k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3628db3e08774883987419a0c4692fcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/155178 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4570afd9e5c435b867d0f1d01bfd935"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/20892 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc2d4337170a40c0a4725aa30dd93f2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/21098 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b1a8ba04b34449da02da3257bc8ecc2"}},"metadata":{}}]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:25:10.714353Z","iopub.execute_input":"2024-09-25T17:25:10.714695Z","iopub.status.idle":"2024-09-25T17:25:10.721600Z","shell.execute_reply.started":"2024-09-25T17:25:10.714661Z","shell.execute_reply":"2024-09-25T17:25:10.720574Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 155178\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 20892\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 21098\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    labels = []\n    for i, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            else:\n                label_ids.append(-100)\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\ntokenized_datasets = data.map(tokenize_and_align_labels, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:25:14.720247Z","iopub.execute_input":"2024-09-25T17:25:14.721108Z","iopub.status.idle":"2024-09-25T17:25:29.497562Z","shell.execute_reply.started":"2024-09-25T17:25:14.721070Z","shell.execute_reply":"2024-09-25T17:25:29.496640Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/155178 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70faa79299134f7b9895c593ebb4b81b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20892 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b431fc473a8546119d52906d74dfb856"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21098 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e74d77132c9943e4975ba0fb583a15b5"}},"metadata":{}}]},{"cell_type":"code","source":"# Load seqeval metric\nmetric = load_metric(\"seqeval\")\n\n# Define compute_metrics function\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_labels = [[label_list[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:25:29.499634Z","iopub.execute_input":"2024-09-25T17:25:29.500321Z","iopub.status.idle":"2024-09-25T17:25:31.343024Z","shell.execute_reply.started":"2024-09-25T17:25:29.500270Z","shell.execute_reply":"2024-09-25T17:25:31.341887Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"730869d3e81644d3b6985235bcc84adb"}},"metadata":{}}]},{"cell_type":"code","source":"# Set up data collator for dynamic padding\ndata_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:25:31.344284Z","iopub.execute_input":"2024-09-25T17:25:31.344706Z","iopub.status.idle":"2024-09-25T17:25:31.349223Z","shell.execute_reply.started":"2024-09-25T17:25:31.344661Z","shell.execute_reply":"2024-09-25T17:25:31.348251Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_scheduler, TrainingArguments, Trainer, EarlyStoppingCallback\n\n# Assuming model, tokenizer, data_collator, tokenized_datasets, and compute_metrics are already defined\n\n# Set up training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./bert_based_dutch_cased_NER_LoRA_sep25\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,  # Weight decay for L2 regularization\n    load_best_model_at_end=True,  # Load the best model when early stopping is triggered\n    metric_for_best_model=\"eval_loss\",  # Metric to monitor for early stopping\n    greater_is_better=False,\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:25:31.351488Z","iopub.execute_input":"2024-09-25T17:25:31.351849Z","iopub.status.idle":"2024-09-25T17:25:31.390287Z","shell.execute_reply.started":"2024-09-25T17:25:31.351814Z","shell.execute_reply":"2024-09-25T17:25:31.389407Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Initialize an optimizer\noptimizer = AdamW(\n    model.parameters(),\n    lr=training_args.learning_rate,\n    weight_decay=training_args.weight_decay\n)\n\n# Define the scheduler (optional, here using a linear scheduler with warmup)\nlr_scheduler = get_scheduler(\n    name=\"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,  # Number of warmup steps (you can change this)\n    num_training_steps=len(tokenized_datasets[\"train\"]) // training_args.per_device_train_batch_size * training_args.num_train_epochs,\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:25:31.393357Z","iopub.execute_input":"2024-09-25T17:25:31.393684Z","iopub.status.idle":"2024-09-25T17:25:31.405207Z","shell.execute_reply.started":"2024-09-25T17:25:31.393645Z","shell.execute_reply":"2024-09-25T17:25:31.404419Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize Trainer with compute_metrics and data collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    optimizers=(optimizer, lr_scheduler),  # Pass the optimizer and scheduler to the Trainer\n   # callbacks=[early_stopping_callback]  # Include early stopping callback\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:25:40.409935Z","iopub.execute_input":"2024-09-25T17:25:40.411063Z","iopub.status.idle":"2024-09-25T17:25:41.454165Z","shell.execute_reply.started":"2024-09-25T17:25:40.411008Z","shell.execute_reply":"2024-09-25T17:25:41.453432Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:25:44.212525Z","iopub.execute_input":"2024-09-25T17:25:44.212884Z","iopub.status.idle":"2024-09-25T18:18:58.494183Z","shell.execute_reply.started":"2024-09-25T17:25:44.212852Z","shell.execute_reply":"2024-09-25T18:18:58.492840Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240925_172605-m373xxp5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dulalsusan773-procit-bv/huggingface/runs/m373xxp5' target=\"_blank\">./bert_based_dutch_cased_NER_LoRA_sep25</a></strong> to <a href='https://wandb.ai/dulalsusan773-procit-bv/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dulalsusan773-procit-bv/huggingface' target=\"_blank\">https://wandb.ai/dulalsusan773-procit-bv/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dulalsusan773-procit-bv/huggingface/runs/m373xxp5' target=\"_blank\">https://wandb.ai/dulalsusan773-procit-bv/huggingface/runs/m373xxp5</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19399' max='58194' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19399/58194 51:51 < 1:43:42, 6.23 it/s, Epoch 1/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.172800</td>\n      <td>No log</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3022\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   3021\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3022\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric_to_check\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n","\u001b[0;31mKeyError\u001b[0m: 'eval_loss'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2487\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2484\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2487\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2490\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2491\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2918\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2915\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2918\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2919\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3024\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   3022\u001b[0m     metric_value \u001b[38;5;241m=\u001b[39m metrics[metric_to_check]\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m-> 3024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   3025\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `metric_for_best_model` training argument is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, which is not found in the evaluation metrics. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3026\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe available evaluation metrics are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Consider changing the `metric_for_best_model` via the TrainingArguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3027\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m   3029\u001b[0m operator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgreater \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgreater_is_better \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mless\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_model_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m operator(metric_value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbest_metric)\n\u001b[1;32m   3034\u001b[0m ):\n","\u001b[0;31mKeyError\u001b[0m: \"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\""],"ename":"KeyError","evalue":"\"The `metric_for_best_model` training argument is set to 'eval_loss', which is not found in the evaluation metrics. The available evaluation metrics are: []. Consider changing the `metric_for_best_model` via the TrainingArguments.\"","output_type":"error"}]},{"cell_type":"code","source":"#Evaluate model\nresults = trainer.evalute()\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.push_to_hub(\"procit008/bert_based_dutch_cased_NER_LoRA_sep25\")\ntokenizer.push_to_hub(\"procit008/bert_based_dutch_cased_NER_LoRA_sep25\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(\"procit008/FacebookAI/xlm-roberta-large-NER_sep25\",\n                  use_auth_token=True,\n                  commit_message=\"basic training\",\n                  private=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\npeft_model_id = \"samwit/bloom-7b1-lora-tagger\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\nmodel = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n# Load the Lora model\nmodel = PeftModel.from_pretrained(model, peft_model_id)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = tokenizer(\"“Training models with PEFT and LoRa is cool” ->: \", return_tensors='pt')\n\nwith torch.cuda.amp.autocast():\n  output_tokens = model.generate(**batch, max_new_tokens=50)\n\nprint('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}